# Effectiveness of Instruction Tuning in Biomedical Language Processing

## Overview

This project explores the effectiveness of instruction tuning in biomedical language processing using the Llama2-MedTuned-Instructions dataset. The dataset is designed for training language models in biomedical NLP tasks, containing approximately 200,000 samples with specific instructions for tasks such as Named Entity Recognition (NER), Relation Extraction (RE), and Medical Natural Language Inference (NLI).

## Dataset

- **Dataset Name:** Llama2-MedTuned-Instructions
- **Dataset Link:** [Llama2-MedTuned-Instructions Dataset](https://huggingface.co/datasets/nlpie/Llama2-MedTuned-Instructions?row=0)
- **Description:** The Llama2-MedTuned-Instructions dataset is tailored for instruction-based learning, providing guidance to language models for various biomedical NLP tasks.

## Models Used

1. **GPT2**
   - **Model Link:** [GPT2 Documentation](https://huggingface.co/docs/transformers/model_doc/gpt2)
   - **Description:** GPT2 serves as one of the language models used in this project. It is fine-tuned on the Llama2-MedTuned-Instructions dataset for biomedical language processing tasks.

2. **GPT-Medium**
   - **Model Link:** [GPT-Medium Model](https://huggingface.co/openai-community/gpt2-medium)
   - **Description:** GPT-Medium, a variant of GPT2, is also employed in this project. It is trained on a medium-sized corpus, enhancing its capabilities for biomedical language understanding.


